{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gMPpRk5D1Sqs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from gensim.models import word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qibfEqVw1Sq8"
      },
      "source": [
        "### Loading the training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "wu6si6oO1Sq8"
      },
      "outputs": [],
      "source": [
        "train_data = \"/content/drive/MyDrive/r8-train-all-terms.txt\"\n",
        "test_data = \"/content/drive/MyDrive/r8-test-all-terms.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN9XemYVc1J3",
        "outputId": "a919563b-ffe8-4b77-9fd0-b05207f3152b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eGf5z34z1Sq9",
        "outputId": "0b38ee5b-471b-4f4d-ee25-247a92bcba87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total examples 5485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "X, y = [], []\n",
        "with open(train_data, \"r\") as infile:\n",
        "    for line in infile:\n",
        "        label, text = line.split(\"\\t\")\n",
        "        X.append(text.split())\n",
        "        y.append(label)\n",
        "X, y = np.array(X), np.array(y)\n",
        "print (\"total examples %s\" % len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A1QNe5qW1Sq-",
        "outputId": "6cc2e67d-7b43-494f-a51e-584046219f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total examples 2189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = [], []\n",
        "with open(test_data, \"r\") as infile:\n",
        "    for line in infile:\n",
        "        label, text = line.split(\"\\t\")\n",
        "        X_test.append(text.split())\n",
        "        y_test.append(label)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "print (\"total examples %s\" % len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hevTROT61Sq_",
        "outputId": "c2d560cf-8680-4661-df7a-e56c113f26d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5485,), (2189,))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c5baDVH_1Sq_",
        "outputId": "64550951-b1b9-426d-8bf3-613d326af43d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['acq', 'crude', 'earn', 'grain', 'interest', 'money-fx', 'ship',\n",
              "        'trade'], dtype='<U8'),\n",
              " array([1596,  253, 2840,   41,  190,  206,  108,  251]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "np.unique(y, return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWbEehhI1SrA"
      },
      "source": [
        "## Using NB methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EARAPKjI1SrB"
      },
      "source": [
        "First, reconstructring the string text for our vectorizers to work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vRvgwVCn1SrC",
        "outputId": "ee6fd85f-31be-4a33-de1a-3ad58f878d11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "champion products ch approves stock split champion products inc said its board of directors approved a two for one stock split of its common shares for shareholders of record as of april the company also said its board voted to recommend to shareholders at the annual meeting april an increase in the authorized capital stock from five mln to mln shares reuter\n",
            "\n",
            "vieille montagne reports loss dividend nil year net loss after exceptional charges mln francs vs profit mln exceptional provisions for closure of viviez electrolysis plant mln francs vs exceptional gain mln sales and services billion francs vs billion proposed net dividend on ordinary shares nil vs francs company s full name is vieille montagne sa vmnb br reuter\n"
          ]
        }
      ],
      "source": [
        "X_text = [\" \".join(val) for val in X]\n",
        "print(X_text[0] + \"\\n\")\n",
        "X_test_text = [\" \".join(val) for val in X_test]\n",
        "print(X_test_text[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QXmQp9P1SrD"
      },
      "source": [
        "#### Using count vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "s4UqVImf1SrD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  #frequency of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fiN17p441SrE",
        "outputId": "034ff71c-8895-46a9-868e-489d7874690e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_features=5000, stop_words='english')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "vect = CountVectorizer(stop_words='english',max_features=5000)\n",
        "vect.fit(X_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23XV4Sz41SrE"
      },
      "source": [
        "Applying the vectorizer to our test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "2NW3S_Xa1SrF"
      },
      "outputs": [],
      "source": [
        "X_train_transformed = vect.transform(X_text)\n",
        "X_test_transformed =vect.transform(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZaWQtx_D1SrF",
        "outputId": "01d5a5be-4f0f-4730-af26-f1b266a14d75",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('champion', 759),\n",
              " ('products', 3488),\n",
              " ('approves', 264),\n",
              " ('stock', 4337),\n",
              " ('split', 4273),\n",
              " ('said', 3974),\n",
              " ('board', 532),\n",
              " ('directors', 1327),\n",
              " ('approved', 263),\n",
              " ('common', 895)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# printing the vocabulary\n",
        "list(vect.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411_lPy11SrG"
      },
      "source": [
        "### Using Bernoulli NB first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "id": "vx8DvC6V1SrG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kM-2Kud01SrG",
        "outputId": "26d6b0f2-8309-4406-8fbe-6a6b9f50bb11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.8736554238833182\n",
            "Test accuracy:  0.8688899040657835\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train_transformed,y)\n",
        "\n",
        "# predict class\n",
        "pred_train_ys = bnb.predict(X_train_transformed)\n",
        "pred_test_ys = bnb.predict(X_test_transformed)\n",
        "\n",
        "# accuracy\n",
        "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISEOoX0P1SrH"
      },
      "source": [
        "### Using Multinomial NB\n",
        " - We expect this to work very well, giving high performance in accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "IY9GHhgy1SrH"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aaDhRXuZ1SrI",
        "outputId": "6b44e9f3-e537-4c25-adbd-f5ed35fd28bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.968094804010939\n",
            "Test accuracy:  0.9657377798081316\n"
          ]
        }
      ],
      "source": [
        "#fit on training data\n",
        "mnb.fit(X_train_transformed, y)\n",
        "\n",
        "# predict class\n",
        "pred_train_ys = mnb.predict(X_train_transformed)\n",
        "pred_test_ys = mnb.predict(X_test_transformed)\n",
        "\n",
        "# accuracy\n",
        "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh5oKT4H1SrI"
      },
      "source": [
        " - As expected, this performed really well\n",
        " - Remember that we used 5000 features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_knqDy1SrJ"
      },
      "source": [
        "## Using our word embeddings approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DPw6hTN1SrJ"
      },
      "source": [
        " We have two options here:\n",
        " \n",
        "1. Use pre-trained word vectors (Glove)\n",
        "\n",
        "2. Train our own vectors\n",
        "\n",
        "We'll explore both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ5U5GnB1SrK"
      },
      "source": [
        "### Loading the pre-trained GloVe word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CrsUmH0z1SrL"
      },
      "outputs": [],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.200d.txt'\n",
        "word2vec_output_file = 'glove.6B.200d.w2vformat.txt'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4pBiPZmP1SrL"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.w2vformat.txt\", binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P81T_o01SrM"
      },
      "source": [
        "### Sentence vector by averaging word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "HriNtyAy1SrM"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dv5riInI1SrM"
      },
      "outputs": [],
      "source": [
        "def sent_vec(sent):\n",
        "    wv_res = np.zeros(glove_model.vector_size)\n",
        "    ctr = 1\n",
        "    for w in sent:\n",
        "        if w in glove_model:\n",
        "            ctr += 1\n",
        "            wv_res += glove_model[w]\n",
        "    wv_res = wv_res/ctr\n",
        "    #return (wv_res, ctr)\n",
        "    return wv_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X1OYzqRQ1SrM"
      },
      "outputs": [],
      "source": [
        "train_doc_vecs = []\n",
        "for doc in X:    \n",
        "    doc_words = [term for term in doc if term not in stop_words]\n",
        "    train_doc_vecs.append(sent_vec(doc_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H1tHV24w1SrN"
      },
      "outputs": [],
      "source": [
        "test_doc_vecs = []\n",
        "for doc in X_test:    \n",
        "    doc_words = [term for term in doc if term not in stop_words]\n",
        "    test_doc_vecs.append(sent_vec(doc_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JazcEf-A1SrN"
      },
      "source": [
        "### Building a predictive model on the averaged word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bgKQ6p1SrN"
      },
      "source": [
        "#### Using a 'simple' logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WSAAiYT61SrO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egd7bzIN1SrO",
        "outputId": "5c2eace0-f6f6-42b5-c955-923b129f90fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=3.5, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "execution_count": 83,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 3.5)\n",
        "logreg.fit(train_doc_vecs,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp0Y7wP31SrO",
        "outputId": "7c0fa011-0058-497f-b705-edfdea1f6379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy:  0.978122151322\n",
            "Test accuracy:  0.968021927821\n"
          ]
        }
      ],
      "source": [
        "pred_train_ys = logreg.predict(train_doc_vecs)\n",
        "pred_test_ys = logreg.predict(test_doc_vecs)\n",
        "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
        "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC9kMPC41SrP"
      },
      "source": [
        "### Training our own wordvectors on the data\n",
        "We'll create a combined text file to train our word vectors - more data is better. Although in this case we would still have just 7.7K instances to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "neghGb6c1SrP",
        "outputId": "bda48e95-be11-4872-a334-6cd51b85fc91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7674"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "X_comb = np.concatenate([X,X_test])\n",
        "len(X_comb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "F-dFubh51SrP",
        "outputId": "c92debcd-e973-46cc-80ba-0893ba468b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fhlbb', 'changes', 'short', 'term', 'discount', 'note', 'rates', 'the', 'federal', 'home', 'loan', 'bank', 'board', 'adjusted', 'the', 'rates', 'on', 'its', 'short', 'term', 'discount', 'notes', 'as', 'follows', 'maturity', 'new', 'rate', 'old', 'rate', 'maturity', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'days', 'pct', 'pct', 'days', 'reuter']\n"
          ]
        }
      ],
      "source": [
        "print(X_comb[6000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "tYpNTZUq1SrQ"
      },
      "outputs": [],
      "source": [
        "w2v = word2vec.Word2Vec(X_comb, window=2, min_count=2, sg = 1, size=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VDbeHk_T1SrQ",
        "outputId": "02b6c67d-9da5-4c84-b5d5-a7b25bba5f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('near', 0.8136610984802246),\n",
              " ('significant', 0.7828431129455566),\n",
              " ('prospects', 0.7772178649902344),\n",
              " ('foreseeable', 0.7758526802062988),\n",
              " ('outlook', 0.7702556252479553)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "w2v.most_similar(\"future\", topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60AmJfdy1SrR"
      },
      "source": [
        "#### Sentence vectors by averaging vectors for words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "-WLvYsNp1SrR"
      },
      "outputs": [],
      "source": [
        "def sent_vec_w2v(sent):\n",
        "    wv_res = np.zeros(w2v.vector_size)\n",
        "    ctr = 1\n",
        "    for w in sent:\n",
        "        if w in w2v:\n",
        "            ctr += 1\n",
        "            wv_res += w2v[w]\n",
        "    wv_res = wv_res/ctr\n",
        "    return wv_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLcyQVfW1SrS"
      },
      "source": [
        "#### Getting the sentence vectors for the test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNWPs1F0k36U",
        "outputId": "a5ab9a1a-9c45-4793-f77c-935f2d1f673c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "79a6Vvtw1SrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8597805-9bf4-4e54-f560-7c9024ec2f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "train_doc_vecs = []\n",
        "for doc in X:    \n",
        "    doc_words = [term for term in doc if term not in stop_words]\n",
        "    train_doc_vecs.append(sent_vec_w2v(doc_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "9U16RUiS1SrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6315c57a-f07c-487b-89cd-2e84a161a7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "test_doc_vecs = []\n",
        "for doc in X_test:    \n",
        "    doc_words = [term for term in doc if term not in stop_words]\n",
        "    test_doc_vecs.append(sent_vec_w2v(doc_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rxjOb421SrT"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "ZNdMv0IE1SrT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "YtqrGu9g1SrT"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression(penalty=\"l2\", random_state=42, C = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iE_n8tGF1SrU",
        "outputId": "15b1e02c-5df9-48fd-b21f-7ea932d03a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=8, random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "logreg.fit(train_doc_vecs,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "b5UZ-hXG1SrU",
        "outputId": "994a6852-f2fb-4102-c3de-4e23d266af66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.9693710118505013\n",
            "Test accuracy:  0.9616263133851074\n"
          ]
        }
      ],
      "source": [
        "pred_train_ys = logreg.predict(train_doc_vecs)\n",
        "pred_test_ys = logreg.predict(test_doc_vecs)\n",
        "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
        "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xE3jgqRhmM26"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}