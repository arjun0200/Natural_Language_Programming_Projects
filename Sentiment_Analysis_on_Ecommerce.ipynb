{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrlyPWP_oSJU"
      },
      "source": [
        "# Analyse E-commerce Product Reviews using NLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1uEXf5yohT4"
      },
      "source": [
        "## **Import Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L9LqWZcoGNv"
      },
      "outputs": [],
      "source": [
        "# Import the generic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import csv\n",
        "\n",
        "# Importing nltk libraries\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Visualization libraries\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image # for world cloud image\n",
        "\n",
        "# Spacy for preprocessing\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Spell checker\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Modelling\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# xgboost\n",
        "import xgboost as xgb\n",
        "#\n",
        "import tensorflow as tf\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# To change date to datetime\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re \n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "import scipy.sparse\n",
        "\n",
        "# Textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Gensim libraries\n",
        "!pip install pyLDAvis\n",
        "from gensim import corpora, models, similarities, matutils\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "# To show all the columns\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "\n",
        "# to pickle dataframe\n",
        "import pickle\n",
        "\n",
        "# Avoid warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Enable logging for gensim - optional but important\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh-bjp7Zo87R"
      },
      "source": [
        "## **Load the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KtUJdIQ7XxBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQqqYLU3o6Uj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/GrammarandProductReviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iilNOUe4pUw1"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvOm_HpOpe0p"
      },
      "source": [
        "# **Text Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38m8KtwepYhJ"
      },
      "outputs": [],
      "source": [
        "print(\"Shape :\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5y1ywCdjdzG"
      },
      "outputs": [],
      "source": [
        "print(\"Columns :\")\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0W9sm7wDryK"
      },
      "source": [
        "### **Rename the Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMdqE9lorYk8"
      },
      "outputs": [],
      "source": [
        "# Rename the column names\n",
        "col_names = df.columns\n",
        "new_col_names = [i.replace(\".\",\"_\") for i in col_names]\n",
        "df.columns = new_col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-j-e_egsDU8"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8LuAhqAsPFQ"
      },
      "outputs": [],
      "source": [
        "print(\"Datatypes :\\n\",df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCUAQ-R4s7gg"
      },
      "outputs": [],
      "source": [
        "print(\"Info :\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMv4675KzLTj"
      },
      "outputs": [],
      "source": [
        "print(\"Missing Value Count :\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kShnhr4AXI_n"
      },
      "source": [
        "### **Percentage of missing values per column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIud5qoMXDNA"
      },
      "outputs": [],
      "source": [
        "print(\"Percentage of missing values :\")\n",
        "print(df.isna().mean().round(4) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpv5L0milPLT"
      },
      "outputs": [],
      "source": [
        "#Visualization of Missing Values\n",
        "\n",
        "# Plot it\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "sns.heatmap(data=df.isnull())\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfJ-Vf7ZtxW0"
      },
      "outputs": [],
      "source": [
        "print(\"Total Missing Value Count : \", df.isnull().sum().values.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGZAurKlonSk"
      },
      "outputs": [],
      "source": [
        "# Drop the columns with less than 20% of values\n",
        "missing_val_threshold = len(df) * .2\n",
        "df.dropna(thresh = missing_val_threshold, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mJKo3ZsozfQ"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkMYY4u1peTK"
      },
      "outputs": [],
      "source": [
        "# Shape of Dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lkLfdAot8y-"
      },
      "outputs": [],
      "source": [
        "# Drop the rows where \"reviews.text\" or \"reviews.date\" feature has Null values\n",
        "df.dropna(subset=['reviews_text','reviews_date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YWkNCWQqa5v"
      },
      "outputs": [],
      "source": [
        "# Shape of Dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe6T6bjOqdNo"
      },
      "outputs": [],
      "source": [
        "print(\"Percentage of missing values :\")\n",
        "print(df.isna().mean().round(4) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrkjj2286f2_"
      },
      "source": [
        "### **Combine Review Text and Title into one**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6-C0nwq6l4O"
      },
      "outputs": [],
      "source": [
        "# Joining Review Text and Title \n",
        "df['Review'] = df['reviews_title'].map(str) + \" \" + df['reviews_text'] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeMx73W-4TwH"
      },
      "source": [
        "### **Lowercasing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqCezFXd4TFm"
      },
      "outputs": [],
      "source": [
        "# Lowercasing the reviews and title column\n",
        "df['Review'] = df['Review'].apply(lambda x : x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMf7feJW53Sq"
      },
      "outputs": [],
      "source": [
        "df['Review'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma4e03wr-azG"
      },
      "source": [
        "### **Remove Punctuation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxryRzEF6Nz2"
      },
      "outputs": [],
      "source": [
        "# Remove punctuation \n",
        "df['Review'] = df['Review'].str.replace('[^\\w\\s]','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE9DdtaKCDGO"
      },
      "outputs": [],
      "source": [
        "df['Review'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH5fBez4_ce2"
      },
      "source": [
        "### **Remove Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc2_8qAR_hqg"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['Review'] = df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YuziifMB-cm"
      },
      "outputs": [],
      "source": [
        "df['Review'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1qbAx4bCI1g"
      },
      "source": [
        "### **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E06DheXrCO3i"
      },
      "outputs": [],
      "source": [
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ux2g5LwEvyx"
      },
      "outputs": [],
      "source": [
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1lTmnwiE5Ww"
      },
      "outputs": [],
      "source": [
        "df['Review']=df['Review'].apply(lambda x: lemmatize_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTglEa6iF_XS"
      },
      "outputs": [],
      "source": [
        "df['Review'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBNLqs1ue2SX"
      },
      "source": [
        "### **Spelling Correction**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review']=df['Review'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
        "df['Review'] = df['Review'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
        "df['Review'] = df['Review'].apply(lambda x: x.strip())"
      ],
      "metadata": {
        "id": "i2jSH921ipSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review'].head(10)"
      ],
      "metadata": {
        "id": "q3aBX90YkrBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "# def autospell(text):\n",
        "#         spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
        "#         return \" \".join(spells)"
      ],
      "metadata": {
        "id": "HIiM7CojoB9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5cmDfZSe8dk"
      },
      "outputs": [],
      "source": [
        "df['Review']=df['Review'].apply(lambda x: ' '.join(spell(word) for word in nltk.word_tokenize(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zprGt26cu1US"
      },
      "outputs": [],
      "source": [
        "#df.to_csv(\"/content/drive/My Drive/Updated_GrammarandProductReviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/Updated_GrammarandProductReviews.csv\")"
      ],
      "metadata": {
        "id": "CwLOAONZAHzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujY0JRdeF-Mc"
      },
      "source": [
        "### **Normalization**\n",
        "https://sentic.net/microtext-normalization.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tae7P4TQ9xq"
      },
      "source": [
        "### **Standarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxItAbX-P33i"
      },
      "source": [
        "### **Noise Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVFvkAN8P9A9"
      },
      "outputs": [],
      "source": [
        "def scrub_words(text):\n",
        "    \"\"\"Basic cleaning of texts.\"\"\"\n",
        "    \n",
        "    # remove html markup\n",
        "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
        "    \n",
        "    #remove non-ascii and digits\n",
        "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
        "    \n",
        "    #remove whitespace\n",
        "    text=text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8MhgSElKsLy"
      },
      "outputs": [],
      "source": [
        "df['Review']=df['Review'].apply(lambda x: scrub_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XLxBjZDN0xD"
      },
      "outputs": [],
      "source": [
        "df['Review'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPM2FHDMS0PN"
      },
      "source": [
        "### **Word Count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV4JgyAJS4kD"
      },
      "outputs": [],
      "source": [
        "df['Review_WC'] = df['Review'].apply(lambda x: len(str(x).split(\" \")))\n",
        "df[['Review_WC','Review']].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52c-6-MyU6bB"
      },
      "outputs": [],
      "source": [
        "# Density Plot and Histogram of all Word Count\n",
        "sns.distplot(df['Review_WC'], hist=True, kde=True, \n",
        "             bins=int(180/5), color = 'darkblue', \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF5mr_XpTTB4"
      },
      "source": [
        "### **Character Count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3sd3yHxTXi_"
      },
      "outputs": [],
      "source": [
        "df['Review_CC'] = df['Review'].str.len() ## this also includes spaces\n",
        "df[['Review_CC','Review']].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Dhvd-aX0s5"
      },
      "outputs": [],
      "source": [
        "# Density Plot and Histogram of all Character Count\n",
        "sns.distplot(df['Review_CC'], hist=True, kde=True, \n",
        "             bins=int(180/5), color = 'darkblue', \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mA3S6cuTuB-"
      },
      "source": [
        "### **Average Word Length**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of0NQ6hwT4Tz"
      },
      "outputs": [],
      "source": [
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "df['Review_AWL'] = df[\"Review\"].apply(lambda x: avg_word(x))\n",
        "df[['Review_AWL','Review']].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mSRVKo1YzU_"
      },
      "outputs": [],
      "source": [
        "# Density Plot and Histogram of Average Word Length\n",
        "sns.distplot(df['Review_AWL'], hist=True, kde=True, \n",
        "             bins=int(180/5), color = 'darkblue', \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD6wiuxPUMpW"
      },
      "source": [
        "### **Top 30 Common Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKuGsFOWxbpQ"
      },
      "outputs": [],
      "source": [
        "# function to plot most frequent terms\n",
        "def freq_words(x, terms = 30):\n",
        "  all_words = ' '.join([text for text in x])\n",
        "  all_words = all_words.split()\n",
        "\n",
        "  fdist = FreqDist(all_words)\n",
        "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
        "\n",
        "  # selecting top 20 most frequent words\n",
        "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
        "  plt.figure(figsize=(20,10))\n",
        "  ax = sns.barplot(data=d, x= \"count\", y = \"word\")\n",
        "  ax.set(ylabel = 'Word')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA7PnYjhxjsW"
      },
      "outputs": [],
      "source": [
        "freq_words(df['Review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2zEpu3_b3Mu"
      },
      "source": [
        "### **Top 30 Rare Words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PABXHEwdxh8z"
      },
      "outputs": [],
      "source": [
        "# function to plot least frequent terms\n",
        "def freq_words(x, terms = 30):\n",
        "  all_words = ' '.join([text for text in x])\n",
        "  all_words = all_words.split()\n",
        "\n",
        "  fdist = FreqDist(all_words)\n",
        "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
        "\n",
        "  # selecting top 20 most frequent words\n",
        "  d = words_df.nsmallest(columns=\"count\", n = terms) \n",
        "  plt.figure(figsize=(20,10))\n",
        "  ax = sns.barplot(data=d, x= \"count\", y = \"word\")\n",
        "  ax.set(ylabel = 'Word')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A2D8Vja0H7R"
      },
      "outputs": [],
      "source": [
        "freq_words(df['Review'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K24VBy1y5zCI"
      },
      "source": [
        "### **Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9csfUKNzp6c"
      },
      "outputs": [],
      "source": [
        "df_tf = (df['Review'][0:1]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "df_tf.columns = ['words','tf']\n",
        "df_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6WnXxnI6RJQ"
      },
      "source": [
        "### **IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNy5ynhR6Vd2"
      },
      "outputs": [],
      "source": [
        "for i,word in enumerate(df_tf['words']):\n",
        "  df_tf.loc[i, 'idf'] = np.log(df_tf.shape[0]/(len(df[df['Review'].str.contains(word)])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWnxAnEr6vSe"
      },
      "outputs": [],
      "source": [
        "df_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWk8IhLH65Gp"
      },
      "source": [
        "### **TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb1sw3tz68dL"
      },
      "outputs": [],
      "source": [
        "df_tf['tfidf'] = df_tf['tf'] * df_tf['idf']\n",
        "df_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nweYD9vy3eR"
      },
      "source": [
        "### **Transforming Reviews Date to Python DateTime Format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODaNlEa6yq_I"
      },
      "outputs": [],
      "source": [
        "# Convert the Rating Date column in datetime format\n",
        "df['reviews_date'] = df['reviews_date'].str.replace(\".000Z\",\"\")\n",
        "df['reviews_date'] = df['reviews_date'].str.replace(\"Z\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEX7mKEzyu1b"
      },
      "outputs": [],
      "source": [
        "date = []\n",
        "for i,j in enumerate(df['reviews_date']): \n",
        "  try :\n",
        "    date.append(datetime.strptime(j, \"%Y-%m-%dT%H:%M:%S\"))\n",
        "  except:\n",
        "    print(i)\n",
        "    j = df['reviews_date'][0]\n",
        "    date.append(datetime.strptime(j, \"%Y-%m-%dT%H:%M:%S\"))\n",
        "\n",
        "  \n",
        "df['reviews_date'] = date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgKS50F8xqXX"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZwyrDGEpZNn"
      },
      "source": [
        "### **Unique Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-PYn10fbWd7"
      },
      "outputs": [],
      "source": [
        "for i in ['brand', 'categories','manufacturer','name','reviews_id','reviews_rating']:\n",
        "  print(\"No. of unique values in %s is : %s\" %(i, df[i].nunique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2tR2kjksjGe"
      },
      "source": [
        "### **Distribution of top 25 reviewed brands**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4aIFDQFs56_"
      },
      "outputs": [],
      "source": [
        "df['brand'] = df['brand'].replace(\"L'oreal Paris\",\"L'Oreal Paris\")\n",
        "df['brand'] = df['brand'].replace(\"Sony\",\"Sony Pictures\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd9RYufIrkHO"
      },
      "outputs": [],
      "source": [
        "df['brand'].value_counts()[0:25].sort_values().plot(kind = 'barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PdZyVdu6KN"
      },
      "source": [
        "### **Distribution of reviewed categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ez2XFTYuK6O"
      },
      "outputs": [],
      "source": [
        "df[\"product_cat\"] = df[\"categories\"].apply(lambda x: x.split(\",\")[0])\n",
        "\n",
        "df['product_cat'] = df['product_cat'].replace(\"Movies\",\"Movies & TV Shows\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Food\",\"Food & Beverage\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Household Chemicals\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Music on CD or Vinyl\",\"Musical Instruments & Karaoke\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Pro Audio\",\"Musical Instruments & Karaoke\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Baby\",\"Kids\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Kids' Rooms\",\"Kids\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Brand Shop\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Featured Brands\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Home Improvement\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Furniture\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Video Games\",\"Sports & Outdoors\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Home\",\"Household Essentials\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Mobile\",\"Electronics\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Photography\",\"Electronics\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Patio & Garden\",\"Accessories\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Gift Finder\",\"Accessories\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Auto & Tires\",\"Accessories\")\n",
        "df['product_cat'] = df['product_cat'].replace(\"Kitchen & Dining\",\"Food & Beverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp5y67JBuwjl"
      },
      "outputs": [],
      "source": [
        "df['product_cat'].value_counts()[0:25].sort_values().plot(kind = 'barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhtVPpdnvN2a"
      },
      "source": [
        "### **Distribution of top 25 reviewed manufactures**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a96a1Z5svMvZ"
      },
      "outputs": [],
      "source": [
        "df['manufacturer'] = df['manufacturer'].replace(\"L'oreal Paris\",\"L'Oreal Paris\")\n",
        "df['manufacturer'] = df['manufacturer'].replace(\"SONY CORP\",\"Sony Pictures\")\n",
        "df['manufacturer'].value_counts()[0:25].sort_values().plot(kind = 'barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q0BxQsNp1M9"
      },
      "source": [
        "### **Distribution of Review Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zafTeGPbp_qg"
      },
      "outputs": [],
      "source": [
        "# Density Plot and Histogram of Reviews Ratings\n",
        "sns.countplot(df['reviews_rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWx6m6O51RKA"
      },
      "source": [
        "### **Year Wise Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzdQPmihgdAB"
      },
      "outputs": [],
      "source": [
        "df['year'], df['day'], df['month'] = df['reviews_date'].dt.year, df['reviews_date'].dt.day, df['reviews_date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG8qKnQvg8iW"
      },
      "outputs": [],
      "source": [
        "df['hour'], df['minute'], df['second'] = df['reviews_date'].dt.hour, df['reviews_date'].dt.minute, df['reviews_date'].dt.second"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUMncxtzN3aB"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['year'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShnZ31Ro5nID"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['year'], hue=df['reviews_rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4P6PKNU6kcA"
      },
      "source": [
        "### **Hour Wise Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfvRJH8w6qOE"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['hour'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-njBp6p660qY"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['hour'], hue=df['reviews_rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73glEjmj6Z-6"
      },
      "source": [
        "### **Category vs Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxlAQYph88zp"
      },
      "outputs": [],
      "source": [
        "df['product_cat'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiSa32nz570a"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['product_cat'], hue=df['reviews_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESOHkaHc8X28"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['reviews_rating'], hue=df['product_cat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyVNONZm8or2"
      },
      "source": [
        "### **Brands vs Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io1b-7kn8t0H"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=df['brand'], hue=df['reviews_rating'], order=df['brand'].value_counts().iloc[:20].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbmC0SEvRAEU"
      },
      "source": [
        "### **Word Clouds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHu8wt12Q6WJ"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords,\n",
        "        max_words=300,\n",
        "        max_font_size=40, \n",
        "        scale=3,\n",
        "        random_state=1    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(15, 15))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "show_wordcloud(df['Review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6onZ7gmRJwu"
      },
      "outputs": [],
      "source": [
        "show_wordcloud(df['reviews_title'])\n",
        "# Great = 10938, great = 3133\n",
        "# Disappointed = 156, disappointed = 75, Disappointing = 50, disappointing = 25, dissapoitned = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWCNe6RzR9fT"
      },
      "outputs": [],
      "source": [
        "# try to tokenize to individual word (uni-gram) - reviews.text\n",
        "split_title = []\n",
        "listCounts = []\n",
        "split_title = [x.split(\" \") for x in df['Review'].astype(str)]\n",
        "big_list = []\n",
        "for x in split_title:\n",
        "    big_list.extend(x)\n",
        "\n",
        "listCounts = pd.Series(big_list).value_counts()\n",
        "\n",
        "wordcloud = WordCloud(background_color='white', max_words=400, max_font_size=40, scale=30,\n",
        "        random_state=1).generate((listCounts[listCounts > 2]).to_string())\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIA845N5SB-7"
      },
      "outputs": [],
      "source": [
        "len(big_list)   # reviews_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H1WjlpfBZfO"
      },
      "source": [
        "### **Genuine Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lz8BTmiSGBh"
      },
      "outputs": [],
      "source": [
        "# on the reviews.didPurchase column, replace 38,886 null fields with \"Null\"\n",
        "df['reviews_didPurchase'].fillna('Null', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1FS_c9HSOBA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "ax = sns.countplot(df['reviews_didPurchase'])\n",
        "ax.set_xlabel(xlabel=\"Shoppers did purchase the product\", fontsize=17)\n",
        "ax.set_ylabel(ylabel='Count of Reviews', fontsize=17)\n",
        "ax.axes.set_title('Number of Genuine Reviews', fontsize=17)\n",
        "ax.tick_params(labelsize=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp___RypSSCd"
      },
      "outputs": [],
      "source": [
        "df['reviews_didPurchase'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brXFYjD1SWb_"
      },
      "outputs": [],
      "source": [
        "# shoppers who did purchased the product and provided the review = 5%\n",
        "print(\"Percentage of genuine reviews :\",368200/(38785 + 28474))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aXRkAjJSe0x"
      },
      "source": [
        "### **Correlation Map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4qJjsK1Sa33"
      },
      "outputs": [],
      "source": [
        "# not much info in the correlation map\n",
        "sns.set(font_scale=1.4)\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.heatmap(df.corr(),cmap='coolwarm',annot=True,linewidths=.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ3SA4Nl8cMX"
      },
      "source": [
        "### **Most Bought Product**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aRgRe8NSkZh"
      },
      "outputs": [],
      "source": [
        "# \"The Foodsaver174 10 Cup Fresh Container - Fac10-000\" is purchased almost 500 times\n",
        "df_genuine = df[df['reviews_didPurchase'] == True]\n",
        "df_genuine['name'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YrfXzgASqJh"
      },
      "outputs": [],
      "source": [
        "df_genuine['name'].value_counts()[0:10].plot(kind ='barh', figsize=[10,6], fontsize=20).invert_yaxis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-sMAiUTDfL-"
      },
      "source": [
        "### **Most purchased Product - 5 Rating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyxh_e7eSxhZ"
      },
      "outputs": [],
      "source": [
        "df_mbp = df_genuine[df_genuine['name'] == 'The Foodsaver174 10 Cup Fresh Container - Fac10-000']\n",
        "df_mbp = df_mbp[df_mbp['reviews_rating']==5]\n",
        "# keep relevant columns only\n",
        "df_mbp = df_mbp[[ 'reviews_rating', 'Review']]\n",
        "df_mbp.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsi5zCcV9PNw"
      },
      "source": [
        "### **Most purchased Product - 1 Rating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbgOwzhHkhaY"
      },
      "outputs": [],
      "source": [
        "# filter most purchased product with 1 star rating\n",
        "df_lvp = df_genuine[df_genuine['name'] == 'The Foodsaver174 10 Cup Fresh Container - Fac10-000']\n",
        "df_lvp = df_lvp[df_lvp['reviews_rating']==1]\n",
        "# keep relevant columns only\n",
        "df_lvp = df_lvp[[ 'reviews_rating', 'Review']]\n",
        "df_lvp.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqINgWag-nzk"
      },
      "source": [
        "# **Training Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ygLU1pA-vE"
      },
      "source": [
        "### **Defining features and target variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScDp0LIq-uT6"
      },
      "outputs": [],
      "source": [
        "x=df['Review']\n",
        "y=df['reviews_rating']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lyruxjmBQ1P"
      },
      "source": [
        "### **Using the n-gram tfidf vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eus3wSvWA9yA"
      },
      "outputs": [],
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 3) )  \n",
        "\n",
        "word_vectorizer.fit(x)\n",
        "train_word_features = word_vectorizer.transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7y7jAPOBeWB"
      },
      "outputs": [],
      "source": [
        "char_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    stop_words='english',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=50000)\n",
        "\n",
        "char_vectorizer.fit(x)\n",
        "train_char_features = char_vectorizer.transform(x)\n",
        "\n",
        "train_features = hstack([train_char_features, train_word_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5N-8blMBtJo"
      },
      "source": [
        "### **Splitting the dataset into train and test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRwJyczdBzqn"
      },
      "outputs": [],
      "source": [
        "seed = 50 \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_features, y, test_size=0.3, random_state=seed)\n",
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_test', X_test.shape)\n",
        "print('y_test', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JmzzijeC10e"
      },
      "source": [
        "# **ML Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2jDqN2aC5vI"
      },
      "source": [
        "### **Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_14O0c3bC_1Z"
      },
      "outputs": [],
      "source": [
        "time1 = time.time()\n",
        "\n",
        "classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=seed, n_jobs=-1)\n",
        "classifier.fit(X_train,y_train)\n",
        "\n",
        "preds1 = classifier.predict(X_test)\n",
        "\n",
        "time_taken = time.time() - time1\n",
        "print('Time Taken: {:.2f} seconds'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj6sJIIlDH6Q"
      },
      "outputs": [],
      "source": [
        "print(\"Random Forest Model accuracy\", accuracy_score(preds1, y_test))\n",
        "print(classification_report(preds1, y_test))\n",
        "print(confusion_matrix(preds1, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zu9b2N-D6Fe"
      },
      "source": [
        "### **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6XHlc8FagX"
      },
      "outputs": [],
      "source": [
        "time1 = time.time()\n",
        "\n",
        "logit = LogisticRegression(C=1, multi_class='ovr')\n",
        "logit.fit(X_train,y_train)\n",
        "preds3 = logit.predict(X_test)\n",
        "\n",
        "time_taken = time.time() - time1\n",
        "print('Time Taken: {:.2f} seconds'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYKunqCBFeVh"
      },
      "outputs": [],
      "source": [
        "print(\"Logistic Regression accuracy\", accuracy_score(preds3, y_test))\n",
        "print(classification_report(preds3, y_test))\n",
        "print(confusion_matrix(preds3, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdOWA13bGD07"
      },
      "source": [
        "# **Deep Learning Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alfWt5ZEHfwt"
      },
      "source": [
        "### **Label the Ratings** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQbpwxe-GJX1"
      },
      "outputs": [],
      "source": [
        "# To classify ratings<4 as sentiment, i.e. replace ratings less than 4 as not happy\n",
        "# label 1= Happy\n",
        "# label 2= Unhappy\n",
        "\n",
        "df['sentiment'] = df['reviews_rating']<4\n",
        "train_text, test_text, train_y, test_y = train_test_split(df['Review'],df['sentiment'],test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGYT0ZnfGVw1"
      },
      "outputs": [],
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "\n",
        "# get the raw text data\n",
        "texts_train = train_text.astype(str)\n",
        "texts_test = test_text.astype(str)\n",
        "\n",
        "# finally, vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "sequences = tokenizer.texts_to_sequences(texts_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxyC89ubGbWb"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 200\n",
        "#pad sequences are used to bring all sentences to same size.\n",
        "# pad sequences with 0s\n",
        "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', x_train.shape)\n",
        "print('Shape of data test tensor:', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFIO7iJ1GevX"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vT3qzgGGkD3"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZl7aoAOCypD"
      },
      "source": [
        "[Optimizers](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdjrwUamGpx1"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, train_y,\n",
        "          batch_size=128,\n",
        "          epochs=1,\n",
        "          validation_data=(x_test, test_y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "cl8uyVPI-LB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRwuHMiKG_Wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ujY0JRdeF-Mc",
        "2Tae7P4TQ9xq"
      ],
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.17"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}